{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Data Preprocessing"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T23:01:24.069258Z","iopub.status.busy":"2024-10-19T23:01:24.068809Z","iopub.status.idle":"2024-10-19T23:02:34.498120Z","shell.execute_reply":"2024-10-19T23:02:34.497081Z","shell.execute_reply.started":"2024-10-19T23:01:24.069209Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading source file: /kaggle/input/wikipedia-en-fr/Wikipedia.en-fr.en\n","Reading target file: /kaggle/input/wikipedia-en-fr/Wikipedia.en-fr.fr\n","Total number of sentence pairs: 818302\n","Initial number of sentence pairs: 818302\n","After removing duplicates: 803704\n","After removing sentences longer than 200 words: 801392\n","After enforcing length ratio <= 1.5: 691348\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_30/1310016663.py:65: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df['ratio'] = df['src_word_count'] / df['tgt_word_count']\n"]},{"name":"stdout","output_type":"stream","text":["Removed sentences with non-printable characters: 126\n","Normalized whitespace and Unicode characters.\n","Removed unwanted symbols, retaining essential punctuation.\n"]}],"source":["import unicodedata\n","import re\n","import string\n","import pandas as pd\n","\n","def read_files(source_path, target_path):\n","    \"\"\"\n","    Reads the source and target files and returns a DataFrame.\n","    \n","    Args:\n","        source_path (str): Path to the source language file.\n","        target_path (str): Path to the target language file.\n","    \n","    Returns:\n","        pd.DataFrame: DataFrame with 'source' and 'target' columns.\n","    \"\"\"\n","    print(f\"Reading source file: {source_path}\")\n","    with open(source_path, 'r', encoding='utf-8') as src_file:\n","        sources = src_file.readlines()\n","    \n","    print(f\"Reading target file: {target_path}\")\n","    with open(target_path, 'r', encoding='utf-8') as tgt_file:\n","        targets = tgt_file.readlines()\n","    \n","    if len(sources) != len(targets):\n","        raise ValueError(\"Source and target files have different number of lines.\")\n","    \n","    print(f\"Total number of sentence pairs: {len(sources)}\")\n","    \n","    # Create DataFrame\n","    data = pd.DataFrame({\n","        'source': [s.strip() for s in sources],\n","        'target': [t.strip() for t in targets]\n","    })\n","    print(\"read_files\", len(data))\n","    return data\n","\n","def clean_data(df, max_words=200, length_ratio=1.5):\n","    \"\"\"\n","    Cleans the DataFrame by removing duplicates, filtering by length, and enforcing length ratio.\n","    \n","    Args:\n","        df (pd.DataFrame): DataFrame with 'source' and 'target' columns.\n","        max_words (int): Maximum number of words allowed in a sentence.\n","        length_ratio (float): Maximum allowed length ratio between source and target.\n","    \n","    Returns:\n","        pd.DataFrame: Cleaned DataFrame.\n","    \"\"\"\n","    print(\"Initial number of sentence pairs:\", len(df))\n","    \n","    # Remove duplicate sentence pairs\n","    df.drop_duplicates(inplace=True)\n","    print(\"After removing duplicates:\", len(df))\n","    \n","    # Calculate word counts\n","    df['src_word_count'] = df['source'].apply(lambda x: len(x.split()))\n","    df['tgt_word_count'] = df['target'].apply(lambda x: len(x.split()))\n","    \n","    # Filter out sentences longer than max_words\n","    df = df[(df['src_word_count'] <= max_words) & (df['tgt_word_count'] <= max_words)]\n","    print(f\"After removing sentences longer than {max_words} words:\", len(df))\n","    \n","    # Calculate length ratio\n","    df['ratio'] = df['src_word_count'] / df['tgt_word_count']\n","    \n","    # Keep sentence pairs within the length_ratio\n","    df = df[(df['ratio'] <= length_ratio) & (df['ratio'] >= 1/length_ratio)]\n","    print(f\"After enforcing length ratio <= {length_ratio}:\", len(df))\n","    \n","    # Drop auxiliary columns\n","    df.drop(['src_word_count', 'tgt_word_count', 'ratio'], axis=1, inplace=True)\n","    \n","    return df\n","\n","def remove_non_printable(df):\n","    \"\"\"\n","    Removes sentences containing non-printable or control characters.\n","    \n","    Args:\n","        df (pd.DataFrame): DataFrame with 'source' and 'target' columns.\n","    \n","    Returns:\n","        pd.DataFrame: Cleaned DataFrame.\n","    \"\"\"\n","    def is_printable(text):\n","        return all(c.isprintable() for c in text)\n","    \n","    initial_count = len(df)\n","    \n","    df = df[df['source'].apply(is_printable)]\n","    df = df[df['target'].apply(is_printable)]\n","    \n","    print(f\"Removed sentences with non-printable characters: {initial_count - len(df)}\")\n","    return df\n","\n","def normalize_text(df):\n","    \"\"\"\n","    Normalizes whitespace and Unicode characters in the DataFrame.\n","    \n","    Args:\n","        df (pd.DataFrame): DataFrame with 'source' and 'target' columns.\n","    \n","    Returns:\n","        pd.DataFrame: Cleaned DataFrame.\n","    \"\"\"\n","    def normalize(text):\n","        text = unicodedata.normalize('NFKC', text)  # Normalize Unicode\n","        text = re.sub(r'\\s+', ' ', text)           # Replace multiple spaces with single space\n","        return text.strip()\n","    \n","    df['source'] = df['source'].apply(normalize)\n","    df['target'] = df['target'].apply(normalize)\n","    \n","    print(\"Normalized whitespace and Unicode characters.\")\n","    return df\n","\n","def remove_unwanted_symbols(df, keep_punct=None):\n","    \"\"\"\n","    Removes all symbols from the text except for the specified punctuation marks.\n","    \n","    Args:\n","        df (pd.DataFrame): DataFrame with 'source' and 'target' columns.\n","        keep_punct (list or set): A collection of punctuation marks to retain.\n","                                 Defaults to a standard set if None.\n","    \n","    Returns:\n","        pd.DataFrame: Cleaned DataFrame with unwanted symbols removed.\n","    \"\"\"\n","    if keep_punct is None:\n","        # Define default punctuation to keep\n","        keep_punct = set(['.', ',', '?', '!', ':', ';', '\"', \"'\", '(', ')', '[', ']', '{', '}', '-', '–', '—'])\n","    \n","    # Create a regex pattern to match unwanted symbols\n","    # We keep alphanumerics, whitespace, and the specified punctuation\n","    allowed_chars = ''.join(keep_punct)\n","    # Escape punctuation for regex if necessary\n","    allowed_chars = re.escape(allowed_chars)\n","    \n","    # Pattern to match any character that is not a word character, whitespace, or allowed punctuation\n","    pattern = f'[^\\w\\s{allowed_chars}]'\n","    \n","    def clean_text(text):\n","        # Remove unwanted symbols\n","        text = re.sub(pattern, '', text)\n","        # Optionally, you can also normalize whitespace here if needed\n","        text = re.sub(r'\\s+', ' ', text).strip()\n","        return text\n","    \n","    # Apply the cleaning function to both 'source' and 'target' columns\n","    df['source'] = df['source'].apply(clean_text)\n","    df['target'] = df['target'].apply(clean_text)\n","    \n","    print(\"Removed unwanted symbols, retaining essential punctuation.\")\n","    return df\n","\n","# Step 1: Read the files\n","df = read_files(\"data-files/original_dataset/opus.wikipedia.en-fr/Wikipedia.en-fr.en\",\"data-files/original_dataset/opus.wikipedia.en-fr/Wikipedia.en-fr.fr\")\n","\n","# Step 2: Clean the data\n","df_cleaned = clean_data(df, max_words=200, length_ratio=1.5)\n","df_cleaned = remove_non_printable(df_cleaned)\n","df_cleaned = normalize_text(df_cleaned)\n","df_cleaned = remove_unwanted_symbols(df_cleaned)"]},{"cell_type":"markdown","metadata":{},"source":["# Getting COMET Score"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T23:02:34.499923Z","iopub.status.busy":"2024-10-19T23:02:34.499569Z","iopub.status.idle":"2024-10-19T23:02:47.967899Z","shell.execute_reply":"2024-10-19T23:02:47.966898Z","shell.execute_reply.started":"2024-10-19T23:02:34.499878Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d729fca7e13445c8bc0c93e31fc41e17","version_major":2,"version_minor":0},"text/plain":["Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"]}],"source":["from comet import download_model, load_from_checkpoint\n","model_path = download_model(\"Unbabel/wmt20-comet-qe-da\")\n","model = load_from_checkpoint(model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T23:02:47.970588Z","iopub.status.busy":"2024-10-19T23:02:47.970256Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Processing COMET Scores:  98%|███████████████████████████▎| 337536/345611 [7:44:34<10:14, 13.15it/s]"]}],"source":["from tqdm import tqdm\n","\n","# Define a function to predict COMET scores with a progress bar\n","def get_comet_scores(df, batch_size=64):\n","    scores = []\n","    \n","    # Create a tqdm progress bar\n","    tqdm_bar = tqdm(total=len(df), desc=\"Processing COMET Scores\", ncols=100)\n","\n","    for i in range(0, len(df), batch_size):\n","        batch = df.iloc[i:i + batch_size]\n","        inputs = [{\"src\": row[\"source\"], \"mt\": row[\"target\"]} for _, row in batch.iterrows()]\n","\n","        # Predict scores for the current batch\n","        model_output = model.predict(inputs, batch_size=batch_size, gpus=1, progress_bar=False)\n","\n","        # Extract the scores from the output\n","        if isinstance(model_output, list) and isinstance(model_output[0], tuple):\n","            batch_scores = model_output[0][1]  # Scores are in the second element of the first tuple\n","        elif 'scores' in model_output:\n","            batch_scores = model_output['scores']\n","        else:\n","            raise ValueError(\"Unexpected model output format\")\n","\n","        scores.extend(batch_scores)\n","\n","        # Update the progress bar by the batch size\n","        tqdm_bar.update(len(batch))\n","\n","    tqdm_bar.close()  # Close the progress bar once done\n","    return scores\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_50_part1 = df_cleaned.iloc[:int(0.25 * len(df_cleaned))]\n","df_50_part2 = df_cleaned.iloc[int(0.25 * len(df_cleaned)):int(0.5 * len(df_cleaned))]\n","\n","# Get COMET scores for the DataFrame\n","df_50_part1['comet_score'] = get_comet_scores(df_50_part1)\n","df_50_part2['comet_score'] = get_comet_scores(df_50_part2)\n","\n","# Save the DataFrame with the new column\n","df_50_part1.to_csv('data-files/original_dataset/dataset_with_comet_Part1.csv', index=False)\n","df_50_part2.to_csv('data-files/original_dataset/dataset_with_comet_Part2.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Removing Sentences from different Language"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","\n","# Load the first CSV file\n","df1 = pd.read_csv('en-fr/dataset_with_comet_Part1.csv')\n","\n","# Load the second CSV file\n","df2 = pd.read_csv('en-fr/dataset_with_comet_Part2.csv')\n","\n","# Combine the two dataframes\n","combined_df = pd.concat([df1, df2])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","from langdetect import detect\n","from langdetect.lang_detect_exception import LangDetectException\n","\n","def detect_language(text):\n","    try:\n","        return detect(text)\n","    except LangDetectException:\n","        return 'unknown'\n","\n","# Apply language detection to the 'text' column\n","# Replace 'text' with the actual column name containing the text you want to analyze\n","combined_df['source_detected_language'] = combined_df['source'].apply(detect_language)\n","combined_df['target_detected_language'] = combined_df['target'].apply(detect_language)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["combined_df = combined_df[combined_df['source_detected_language'] == 'en']\n","combined_df = combined_df[combined_df['target_detected_language'] == 'fr']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Sort the dataframe by comet_score in descending order\n","df_sorted = combined_df.sort_values(by='comet_score', ascending=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset Splitting"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Shuffle the dataset\n","df_shuffled = df_sorted.sample(frac=1, random_state=42).reset_index(drop=True)\n","\n","# Split the data into train+validation and test sets (80% / 20%)\n","train, test = train_test_split(df_shuffled, test_size=0.2307692, random_state=42)\n","\n","# Split the train+validation set into train and validation sets (75% / 25%, which is 60% / 20% of the total)\n","test, val = train_test_split(test, test_size=0.50, random_state=42)\n","\n","# Save the splits to CSV files\n","train.to_csv('data-files/filtered_dataset/train.csv', index=False)\n","val.to_csv('data-files/filtered_dataset/validation.csv', index=False)\n","test.to_csv('data-files/filtered_dataset/test.csv', index=False)\n","\n","# Print the sizes of each split\n","print(f\"Total samples: {len(df_shuffled)}\")\n","print(f\"Train samples: {len(train)} ({len(train)/len(df_shuffled)*100:.2f}%)\")\n","print(f\"Validation samples: {len(val)} ({len(val)/len(df_shuffled)*100:.2f}%)\")\n","print(f\"Test samples: {len(test)} ({len(test)/len(df_shuffled)*100:.2f}%)\")"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5890439,"sourceId":9645301,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
